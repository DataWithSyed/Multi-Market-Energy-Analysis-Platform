{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db69841-091b-475b-a89b-fe3cc7089072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "REQUIRED_FILES = {\n",
    "    'price.csv': ['Timestamp', 'Market_Price_EUR'],\n",
    "    'demand.csv': ['Timestamp', 'Load_Demand_MW'],\n",
    "    'solar.csv': ['Timestamp', 'Solar_Gen_MW'],\n",
    "    'wind.csv': ['Timestamp', 'Wind_Gen_MW'],\n",
    "    'nuclear.csv': ['Timestamp', 'Nuclear_Gen_MW'],\n",
    "    'gas.csv': ['Timestamp', 'Gas_Price_EUR'],\n",
    "    'coal.csv': ['Timestamp', 'Coal_Price_EUR'],\n",
    "    'oil.csv': ['Timestamp', 'Oil_Price_EUR'],\n",
    "    'fuelcell.csv': ['Timestamp', 'FuelCell_Gen_MW'],\n",
    "    'ocean.csv': ['Timestamp', 'Ocean_Gen_MW'],\n",
    "    'geothermal.csv': ['Timestamp', 'GeoThermal_Gen_MW'],\n",
    "    'temp.csv': ['Timestamp', 'Ambient_Temp']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d617da97-bd46-4bbe-b8f6-dc5d6479504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. BESS AND STOCHASTIC OPTIMIZATION CONSTANTS ---\n",
    "\n",
    "BESS_CAPACITY = 50.0   # MWh\n",
    "MAX_C_RATE = 0.5       # 0.5C rate (25 MW)\n",
    "RT_AFRR_PRICE = 250.0  # €/MWh for ancillary services revenue\n",
    "DA_ARBITRAGE_THRESHOLD = 0.1 # 10% deviation for arbitrage decision\n",
    "HOURS_TO_FORECAST = 24 # The length of the prediction window\n",
    "NUM_SCENARIOS = 100    # Number of price paths for stochastic modeling\n",
    "CVAR_ALPHA = 0.05      # 5% for P99/CVaR calculation\n",
    "LAMBDA = 0.0001        # Risk Aversion Parameter (tunes P99 strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42055ec2-abd1-476e-b083-8a20ff938aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. CORE FUNCTIONS (Data Ingestion and Cleaning) ---\n",
    "\n",
    "def create_and_load_data():\n",
    "    \"\"\"\n",
    "    SIMULATES the upload process by creating dummy CSV files and reading them back.\n",
    "    For real use, this function would be replaced by user file I/O.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 1. MARKET DATA INGESTION SIMULATION ---\")\n",
    "    print(\"Please ensure your files are named correctly and saved in the same folder.\")\n",
    "    print(\"Files created/loaded (MUST contain a 'Timestamp' column):\\n\")\n",
    "    \n",
    "    data_frames = {}\n",
    "    total_hours = 90 * 24 # 90 days of hourly data\n",
    "\n",
    "    for filename, cols in REQUIRED_FILES.items():\n",
    "        df_data = {}\n",
    "        dates = pd.date_range(start='2024-01-01', periods=total_hours, freq='h')\n",
    "        df_data[cols[0]] = dates\n",
    "\n",
    "        if 'Price' in cols[1] or 'Price' in filename:\n",
    "            # Price/Cost data (Highly Volatile)\n",
    "            base = 50 + 50 * np.sin(dates.dayofyear * 2 * np.pi / 365)\n",
    "            df_data[cols[1]] = np.maximum(5, base + np.random.normal(0, 15, total_hours))\n",
    "            \n",
    "            # Introduce missing data for cleaning demonstration\n",
    "            df_data[cols[1]].iloc[100:105] = np.nan \n",
    "\n",
    "        elif 'Gen_MW' in cols[1]:\n",
    "            # Generation data (Cyclical/Seasonal)\n",
    "            if 'Solar' in cols[1]:\n",
    "                hourly = 100 * np.maximum(0, np.sin(dates.hour * np.pi / 12)) \n",
    "                seasonal = 0.5 + 0.5 * np.sin(dates.dayofyear * 2 * np.pi / 365)\n",
    "                df_data[cols[1]] = np.maximum(0, hourly * seasonal + np.random.normal(0, 5, total_hours))\n",
    "            else: # Wind/Nuclear/Other base gen\n",
    "                df_data[cols[1]] = 50 + 20 * np.random.rand(total_hours)\n",
    "                \n",
    "        elif 'Load' in cols[1]:\n",
    "            # Demand data (Hourly/Daily peaks)\n",
    "            df_data[cols[1]] = 500 + 300 * np.maximum(0, np.sin(dates.hour * np.pi / 12)) + np.random.normal(0, 50, total_hours)\n",
    "        \n",
    "        elif 'Temp' in cols[1]:\n",
    "            # Temperature data\n",
    "            df_data[cols[1]] = 10 + 15 * np.sin(dates.dayofyear * 2 * np.pi / 365) + np.random.normal(0, 2, total_hours)\n",
    "\n",
    "        df = pd.DataFrame(df_data).set_index(cols[0])\n",
    "        # df.to_csv(filename) # Uncomment to actually save files\n",
    "        data_frames[filename] = df\n",
    "        print(f\"   - {filename}: Loaded column '{cols[1]}'\")\n",
    "        \n",
    "    return data_frames\n",
    "\n",
    "def load_and_clean_data(data_frames):\n",
    "    \"\"\"Combines all data, handles missing values (FFILL), and performs feature engineering.\"\"\"\n",
    "    \n",
    "    print(\"\\n--- 2. DATA RELIABILITY & FEATURE ENGINEERING ---\")\n",
    "    \n",
    "    # 1. Merge all DataFrames on the Index (Timestamp)\n",
    "    master_df = pd.DataFrame(index=data_frames[list(data_frames.keys())[0]].index)\n",
    "    \n",
    "    for df in data_frames.values():\n",
    "        master_df = master_df.merge(df, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "    # 2. Data Cleaning: Forward-Fill (FFILL) Missing Values\n",
    "    print(f\"   - Initial Missing Count (before cleaning): {master_df.isnull().sum().sum()}\")\n",
    "    master_df.ffill(inplace=True)\n",
    "    \n",
    "    # Drop any remaining NaNs (e.g., missing data at the very start of the series)\n",
    "    master_df.dropna(inplace=True)\n",
    "    print(f\"   - Final Missing Count (after FFILL): {master_df.isnull().sum().sum()}\")\n",
    "\n",
    "    # 3. Feature Engineering\n",
    "    master_df['Hour_of_Day'] = master_df.index.hour\n",
    "    master_df['Day_of_Week'] = master_df.index.dayofweek\n",
    "    \n",
    "    # Consolidated Net Load Calculation (Crucial feature for forecasting)\n",
    "    gen_cols = [c for c in master_df.columns if 'Gen_MW' in c]\n",
    "    master_df['Total_Gen_MW'] = master_df[gen_cols].sum(axis=1)\n",
    "    \n",
    "    master_df['Net_Load_MW'] = master_df['Load_Demand_MW'] - master_df['Total_Gen_MW']\n",
    "    \n",
    "    # Isolate relevant columns for the final analysis (for the last 90 days)\n",
    "    analysis_df = master_df.iloc[-HOURS_TO_FORECAST * 90:].copy()\n",
    "    print(f\"   - Final Dataset Size: {len(analysis_df)} hourly records.\")\n",
    "    \n",
    "    return analysis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc305c46-4e6e-41ba-aa26-5b2ad0771f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. CORE FUNCTIONS (Forecasting and Optimization) ---\n",
    "\n",
    "def run_forecasting(df):\n",
    "    \"\"\"Trains a simple Linear Regression model on multivariate data and forecasts the next 24h.\"\"\"\n",
    "    \n",
    "    # Prepare features and target (Focus on 3 main drivers of Net Load)\n",
    "    features = ['Net_Load_MW', 'Ambient_Temp', 'Hour_of_Day']\n",
    "    target = 'Market_Price_EUR'\n",
    "    \n",
    "    # Handle the 'Ambient_Temp' column, which might be missing in simulation\n",
    "    if 'Ambient_Temp' not in df.columns:\n",
    "        df['Ambient_Temp'] = df.index.hour.map(lambda x: 15 + 10 * np.sin(x * np.pi / 12))\n",
    "        features = ['Net_Load_MW', 'Hour_of_Day']\n",
    "\n",
    "    X = df[features].values\n",
    "    y = df[target].values\n",
    "\n",
    "    # Scale data for better stability (using MinMaxScaler)\n",
    "    scaler_X = MinMaxScaler()\n",
    "    X_scaled = scaler_X.fit_transform(X)\n",
    "    \n",
    "    # Train the Model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_scaled, y)\n",
    "    \n",
    "    # Create the X_forecast set (assuming next 24 hours follow the average Net Load cycle)\n",
    "    last_load = df['Net_Load_MW'].iloc[-1]\n",
    "    \n",
    "    X_forecast = []\n",
    "    for h in range(HOURS_TO_FORECAST):\n",
    "        # Simplistic forward projection: Last load + hourly cycle adjustment\n",
    "        load_proj = last_load + 50 * np.sin(2 * np.pi * (h + 12) / 24)\n",
    "        temp_proj = df['Ambient_Temp'].iloc[-24:].mean() # Use average temp\n",
    "        hour_proj = (df.index[-1].hour + h + 1) % 24\n",
    "        \n",
    "        if 'Ambient_Temp' in features:\n",
    "            X_forecast.append([load_proj, temp_proj, hour_proj])\n",
    "        else:\n",
    "            X_forecast.append([load_proj, hour_proj])\n",
    "\n",
    "    # Scale the forecast inputs using the *trained* scaler\n",
    "    X_forecast_scaled = scaler_X.transform(np.array(X_forecast))\n",
    "    \n",
    "    # Predict the next 24 prices\n",
    "    forecast_prices = np.maximum(5.0, model.predict(X_forecast_scaled))\n",
    "    \n",
    "    return forecast_prices, df[target].iloc[-HOURS_TO_FORECAST:].values\n",
    "\n",
    "def run_bess_stochastic_optimization(base_prices):\n",
    "    \"\"\"\n",
    "    Implements the core stochastic optimization for P50 (Risk-Neutral) and P99 (Risk-Averse)\n",
    "    using the Expected Profit and CVaR metrics, respectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    def calculate_cvar(profits):\n",
    "        \"\"\"Calculates CVaR (Average of the worst CVAR_ALPHA profits).\"\"\"\n",
    "        profits_sorted = np.sort(profits)\n",
    "        tail_size = max(1, int(len(profits) * CVAR_ALPHA))\n",
    "        return profits_sorted[:tail_size].mean()\n",
    "\n",
    "    def generate_scenarios(prices):\n",
    "        \"\"\"Generates RT price paths based on base prices and a volatility factor.\"\"\"\n",
    "        scenarios = []\n",
    "        NE_DRIVER = prices.mean() * 0.1\n",
    "        for s in range(NUM_SCENARIOS):\n",
    "            scenario_prices = []\n",
    "            for h in range(HOURS_TO_FORECAST):\n",
    "                ne_error = np.random.normal(0, 1)\n",
    "                rt_volatility = NE_DRIVER * ne_error * 0.25 \n",
    "                price = prices[h] + rt_volatility\n",
    "                scenario_prices.append(np.maximum(5, price))\n",
    "            scenarios.append(scenario_prices)\n",
    "        return scenarios\n",
    "\n",
    "    def evaluate_profit(scenario_prices, capacity_reserved):\n",
    "        \"\"\"Calculates total profit for one scenario.\"\"\"\n",
    "        capacity_arb = BESS_CAPACITY - capacity_reserved\n",
    "        P_arb_max = MAX_C_RATE * capacity_arb \n",
    "        soc_arb = capacity_arb * 0.5\n",
    "        total_arb_profit = 0\n",
    "        \n",
    "        avg_price = np.mean(scenario_prices)\n",
    "        \n",
    "        for price_rt in scenario_prices:\n",
    "            price_da = avg_price # Simplified assumption: DA price is the average price\n",
    "            \n",
    "            # Dispatch decision based on DA threshold\n",
    "            if price_da < avg_price * (1 - DA_ARBITRAGE_THRESHOLD) and soc_arb < capacity_arb:\n",
    "                charge_power = min(capacity_arb - soc_arb, P_arb_max)\n",
    "                total_arb_profit -= charge_power * price_rt * 0.95 # Cost realized at RT\n",
    "                soc_arb += charge_power * 0.95 \n",
    "            elif price_da > avg_price * (1 + DA_ARBITRAGE_THRESHOLD) and soc_arb > 0:\n",
    "                discharge_power = min(soc_arb, P_arb_max)\n",
    "                total_arb_profit += discharge_power * price_rt * 0.95 # Revenue realized at RT\n",
    "                soc_arb -= discharge_power\n",
    "        \n",
    "        total_rt_profit = capacity_reserved * RT_AFRR_PRICE * HOURS_TO_FORECAST\n",
    "        return total_arb_profit + total_rt_profit\n",
    "\n",
    "    # Generate Scenarios\n",
    "    scenarios = generate_scenarios(base_prices)\n",
    "    RESERVATION_STEPS = np.linspace(0, BESS_CAPACITY, 11)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Optimization Loop\n",
    "    for capacity_reserved in RESERVATION_STEPS:\n",
    "        profits = np.array([evaluate_profit(s, capacity_reserved) for s in scenarios])\n",
    "        \n",
    "        expected_profit = np.mean(profits)\n",
    "        cvar = calculate_cvar(profits)\n",
    "        risk_adjusted_profit = expected_profit - LAMBDA * abs(cvar) # Risk Mitigation Strategy\n",
    "\n",
    "        results.append({\n",
    "            'Reserved_MWh': capacity_reserved,\n",
    "            'Expected_Profit': expected_profit,\n",
    "            'CVaR_Profit': cvar,\n",
    "            'Risk_Adjusted_Profit': risk_adjusted_profit\n",
    "        })\n",
    "        \n",
    "    optimization_df = pd.DataFrame(results)\n",
    "\n",
    "    # Find optimal points (P50 and P99)\n",
    "    optimal_p50 = optimization_df.loc[optimization_df['Expected_Profit'].idxmax()]\n",
    "    optimal_p99 = optimization_df.loc[optimization_df['Risk_Adjusted_Profit'].idxmax()]\n",
    "\n",
    "    return optimization_df, optimal_p50, optimal_p99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75925081-94c3-4090-8e43-68e70b52b0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. MARKET DATA INGESTION SIMULATION ---\n",
      "Please ensure your files are named correctly and saved in the same folder.\n",
      "Files created/loaded (MUST contain a 'Timestamp' column):\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 115\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Execute the main analysis\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[43mmain_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mmain_analysis\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain_analysis\u001b[39m():    \n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Step 1: Simulate File Upload and Load Data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     data_frames = \u001b[43mcreate_and_load_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Step 2: Clean and Engineer Features\u001b[39;00m\n\u001b[32m      8\u001b[39m     master_df = load_and_clean_data(data_frames)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mcreate_and_load_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     23\u001b[39m     df_data[cols[\u001b[32m1\u001b[39m]] = np.maximum(\u001b[32m5\u001b[39m, base + np.random.normal(\u001b[32m0\u001b[39m, \u001b[32m15\u001b[39m, total_hours))\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# Introduce missing data for cleaning demonstration\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[43mdf_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m[\u001b[32m100\u001b[39m:\u001b[32m105\u001b[39m] = np.nan \n\u001b[32m     28\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mGen_MW\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m cols[\u001b[32m1\u001b[39m]:\n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m# Generation data (Cyclical/Seasonal)\u001b[39;00m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mSolar\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m cols[\u001b[32m1\u001b[39m]:\n",
      "\u001b[31mAttributeError\u001b[39m: 'Index' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "# --- 5. MAIN EXECUTION AND REPORTING ---\n",
    "\n",
    "def main_analysis():    \n",
    "    # Step 1: Simulate File Upload and Load Data\n",
    "    data_frames = create_and_load_data()\n",
    "    \n",
    "    # Step 2: Clean and Engineer Features\n",
    "    master_df = load_and_clean_data(data_frames)\n",
    "    \n",
    "    # Step 3: Forecasting\n",
    "    print(\"\\n--- 3. PRICE FORECASTING ---\")\n",
    "    forecast_prices, actual_prices = run_forecasting(master_df)\n",
    "    print(f\"   - Next {HOURS_TO_FORECAST} hour prices forecasted.\")\n",
    "    \n",
    "    # Step 4: BESS Stochastic Optimization (P50/P99)\n",
    "    print(\"\\n--- 4. BESS STOCHASTIC OPTIMIZATION ---\")\n",
    "    optimization_df, optimal_p50, optimal_p99 = run_bess_stochastic_optimization(forecast_prices)\n",
    "    print(f\"   - Optimal Risk-Neutral (P50) Reserve: {optimal_p50['Reserved_MWh']:.2f} MWh\")\n",
    "    print(f\"   - Optimal Risk-Averse (P99) Reserve: {optimal_p99['Reserved_MWh']:.2f} MWh\")\n",
    "    \n",
    "    # Step 5: Reporting and Graphing\n",
    "    \n",
    "    # Summary Metrics\n",
    "    print(\"\\n--- 5. PROFITABILITY AND STRATEGY SUMMARY ---\")\n",
    "    print(f\"Risk-Neutral (P50) Strategy (Max E[Profit]):\")\n",
    "    print(f\"   Reserved for aFRR: {optimal_p50['Reserved_MWh']:.1f} MWh | E[Profit]: {optimal_p50['Expected_Profit']:,.0f} €\")\n",
    "    print(f\"Risk-Averse (P99) Strategy (Max Risk-Adj. Profit):\")\n",
    "    print(f\"   Reserved for aFRR: {optimal_p99['Reserved_MWh']:.1f} MWh | E[Profit]: {optimal_p99['Expected_Profit']:,.0f} €\")\n",
    "    print(f\"   CVaR (Worst 5% Average Profit): {optimal_p99['CVaR_Profit']:,.0f} €\")\n",
    "    \n",
    "    generate_all_graphs(master_df, actual_prices, forecast_prices, optimization_df)\n",
    "    \n",
    "def generate_all_graphs(df, actual, forecast, opt_df):\n",
    "    \"\"\"Generates all required visualizations.\"\"\"\n",
    "    \n",
    "    print(\"\\n--- 6. GRAPH GENERATION ---\")\n",
    "    \n",
    "    # --- Graph 1: Actual vs. Predicted Energy Prices ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    full_series = np.concatenate([actual, forecast])\n",
    "    actual_labels = [f'H{i}' for i in range(HOURS_TO_FORECAST)]\n",
    "    forecast_labels = [f'F{i}' for i in range(HOURS_TO_FORECAST)]\n",
    "    \n",
    "    plt.plot(actual_labels, actual, label='Actual Historical Price', marker='o', color='blue', linestyle='-')\n",
    "    plt.plot(forecast_labels, forecast, label='Predicted Price (Next 24h)', marker='x', color='red', linestyle='--')\n",
    "    plt.axvline(x='F0', color='gray', linestyle=':', linewidth=2, label='Forecast Start')\n",
    "    \n",
    "    plt.title('Actual vs. Predicted Energy Prices (Forecasting Reliability)')\n",
    "    plt.xlabel('Time (Hours)')\n",
    "    plt.ylabel('Market Price (€/MWh)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('graph_1_price_forecast.png')\n",
    "    \n",
    "    # --- Graph 2: Optimization Decision (P50/P99 Frontier) ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(opt_df['Reserved_MWh'], opt_df['Expected_Profit'], marker='o', label='Expected Profit (P50)', color='green')\n",
    "    plt.plot(opt_df['Reserved_MWh'], opt_df['CVaR_Profit'], marker='s', label='CVaR Profit (P99 - Worst Case)', color='red')\n",
    "    \n",
    "    # Mark Optimal P50 Point\n",
    "    opt_p50 = opt_df.loc[opt_df['Expected_Profit'].idxmax()]\n",
    "    plt.axvline(opt_p50['Reserved_MWh'], color='green', linestyle='--', alpha=0.5, label=f\"Optimal P50 Reserve ({opt_p50['Reserved_MWh']:.1f} MWh)\")\n",
    "    \n",
    "    # Mark Optimal P99 Point\n",
    "    opt_p99 = opt_df.loc[opt_df['Risk_Adjusted_Profit'].idxmax()]\n",
    "    plt.axvline(opt_p99['Reserved_MWh'], color='red', linestyle='--', alpha=0.5, label=f\"Optimal P99 Reserve ({opt_p99['Reserved_MWh']:.1f} MWh)\")\n",
    "    \n",
    "    plt.title('Optimal Capacity Reservation Schedule (P50 vs P99)')\n",
    "    plt.xlabel('Capacity Reserved for RT/Flexibility (MWh)')\n",
    "    plt.ylabel('Profit (€)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('graph_2_bess_decision.png')\n",
    "    \n",
    "    # --- Graph 3: Correlation Heatmap (Market Dependencies) ---\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    corr_cols = ['Market_Price_EUR', 'Net_Load_MW', 'Solar_Gen_MW', 'Gas_Price_EUR', 'Load_Demand_MW']\n",
    "    corr_df = df[corr_cols].corr()\n",
    "    \n",
    "    sns.heatmap(corr_df, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "    plt.title('Correlation Heatmap (Market Dependencies)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('graph_3_correlation_heatmap.png')\n",
    "    \n",
    "    # --- Graph 4: Total Generation Mix Breakdown ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    gen_cols = [c for c in df.columns if 'Gen_MW' in c and c != 'Total_Gen_MW']\n",
    "    gen_mix = df[gen_cols].mean()\n",
    "    \n",
    "    plt.pie(gen_mix, labels=gen_mix.index, autopct='%1.1f%%', startangle=90, colors=plt.cm.Spectral(np.linspace(0, 1, len(gen_mix))))\n",
    "    plt.title('Average Generation Mix Breakdown')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('graph_4_generation_mix.png')\n",
    "    \n",
    "    # --- Graph 5: Load vs. Net Load Profiles (Flexibility Need) ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Use the last 7 days for clear visualization\n",
    "    last_week = df.iloc[-24*7:] \n",
    "    \n",
    "    plt.plot(last_week['Load_Demand_MW'].values, label='Gross Load Demand', color='orange', alpha=0.7)\n",
    "    plt.plot(last_week['Net_Load_MW'].values, label='Net Load (Demand - Renewables)', color='blue')\n",
    "    \n",
    "    plt.title('Load Profiles vs. Net Load (Flexibility Need)')\n",
    "    plt.xlabel('Hour Index (Last 7 Days)')\n",
    "    plt.ylabel('Power (MW)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('graph_5_net_load_profile.png')\n",
    "\n",
    "# Execute the main analysis\n",
    "if __name__ == '__main__':\n",
    "    main_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
